"""
AI Provider Proxy Server
========================

FastAPI ä»£ç†æœåŠ¡ï¼Œè§£å†³æµè§ˆå™¨ CORS é™åˆ¶ã€‚

ç«¯ç‚¹:
    /test   - æµ‹è¯• AI Provider Model è¿é€šæ€§
    /chat   - å‘é€èŠå¤©è¯·æ±‚
    /models - è·å–æ¨¡å‹åˆ—è¡¨

å¯åŠ¨:
    python server.py
    # æˆ–
    uvicorn server:app --reload --port 8000
"""

from typing import Dict, List, Optional
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import httpx
import time

from models import (
    ChatRequest, FetchModelsRequest,
    TestConnectionResponse, ChatResponse, FetchModelsResponse,
)
from strategies import get_strategy, STRATEGY_REGISTRY

# ============================================================================
# é…ç½®å¸¸é‡
# ============================================================================
    
TIMEOUT_CHAT = 100.0      # èŠå¤©è¯·æ±‚è¶…æ—¶ (ç§’)
TIMEOUT_MODELS = 60.0    # æ¨¡å‹åˆ—è¡¨è¯·æ±‚è¶…æ—¶ (ç§’)

# ============================================================================
# FastAPI åº”ç”¨
# ============================================================================

app = FastAPI(title="AI Provider Proxy", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173", "http://localhost:5174", "http://127.0.0.1:5174", "http://localhost:3000", "http://127.0.0.1:3000", "https://tombcato.github.io"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============================================================================
# æ ¸å¿ƒé€»è¾‘
# ============================================================================

async def send_chat_request(api_format: str, base_url: str, api_key: str, model: str, messages: List[Dict[str, str]], max_tokens: Optional[int]):
    """å‘é€èŠå¤©è¯·æ±‚"""
    strategy = get_strategy(api_format)
    async with httpx.AsyncClient(timeout=TIMEOUT_CHAT) as client:
        return await strategy.execute(client, base_url, api_key, model, messages, max_tokens)


def format_error(e: Exception) -> str:
    """æ ¼å¼åŒ–é”™è¯¯ä¿¡æ¯"""
    if isinstance(e, httpx.HTTPStatusError):
        return f"HTTP {e.response.status_code}: {e.response.text[:500]}"
    elif isinstance(e, httpx.TimeoutException):
        return f"è¯·æ±‚è¶…æ—¶ ({int(TIMEOUT_CHAT)}ç§’)"
    return str(e)




# ============================================================================
# API ç«¯ç‚¹
# ============================================================================

@app.get("/", tags=["Health"])
async def root():
    return {
        "status": "running",
        "supported_formats": list(STRATEGY_REGISTRY.keys()),
        "endpoints": ["/test", "/chat", "/models"],
    }


@app.post("/test", response_model=TestConnectionResponse, tags=["Proxy"])
async def test_connection(req: ChatRequest):
    """æµ‹è¯•è¿é€šæ€§"""
    start = time.time()
    try:
        await send_chat_request(req.api_format, req.base_url, req.api_key, req.model, [{"role": "user", "content": "Hi"}], None)
        return TestConnectionResponse(success=True, latency_ms=int((time.time() - start) * 1000), message="è¿æ¥æˆåŠŸ")
    except Exception as e:
        return TestConnectionResponse(success=False, latency_ms=0, message=format_error(e))


@app.post("/chat", response_model=ChatResponse, tags=["Proxy"])
async def chat(req: ChatRequest):
    """å‘é€èŠå¤©è¯·æ±‚"""
    if not req.messages:
        return ChatResponse(success=False, message="messages ä¸èƒ½ä¸ºç©º")
    
    start = time.time()
    try:
        result = await send_chat_request(req.api_format, req.base_url, req.api_key, req.model, req.messages, req.max_tokens)
        return ChatResponse(
            success=True, content=result.content, model=result.model, usage=result.usage,
            latency_ms=int((time.time() - start) * 1000), raw_response=result.raw_response
        )
    except Exception as e:
        return ChatResponse(success=False, latency_ms=int((time.time() - start) * 1000), message=format_error(e))


@app.post("/models", response_model=FetchModelsResponse, tags=["Proxy"])
async def fetch_models(req: FetchModelsRequest):
    """è·å–æ¨¡å‹åˆ—è¡¨"""
    try:
        strategy = get_strategy(req.api_format)
        if not strategy.supports_models_api:
            return FetchModelsResponse(success=False, models=[], message=f"{req.api_format} ä¸æ”¯æŒåŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨")
        
        async with httpx.AsyncClient(timeout=TIMEOUT_MODELS) as client:
            models = await strategy.fetch_models(client, req.base_url, req.api_key or "")
            
            # æŒ‰åˆ›å»ºæ—¶é—´å€’åºæ’åˆ—ï¼ˆæ–°çš„åœ¨å‰ï¼‰
            models_sorted = sorted(
                models,
                key=lambda m: (m.get('created', 0) if isinstance(m, dict) else 0, m.get('id', '')),
                reverse=True
            )
            
            return FetchModelsResponse(success=True, models=models_sorted)
    except Exception as e:
        return FetchModelsResponse(success=False, models=[], message=format_error(e))


# ============================================================================
# å¯åŠ¨
# ============================================================================

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ AI Provider Proxy @ http://localhost:8000")
    print(f"ğŸ“‹ Formats: {', '.join(STRATEGY_REGISTRY.keys())}")
    uvicorn.run(app, host="0.0.0.0", port=8000)
